{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpBVeU0XX8Uk"
   },
   "source": [
    "<h1>12Ïû• ÏÉùÏÑ± Î™®Îç∏ ÎØ∏ÏÑ∏ ÌäúÎãùÌïòÍ∏∞</h1>\n",
    "<i>ÏÉùÏÑ± LLMÏùÑ ÎØ∏ÏÑ∏ ÌäúÎãùÌïòÍ∏∞ ÏúÑÌïú Îëê Îã®Í≥Ñ Ï†ëÍ∑º Î∞©ÏãùÏóê ÎåÄÌïú ÌÉêÌóò</i>\n",
    "\n",
    "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter12.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÄ <[Ìï∏Ï¶àÏò® LLM](https://tensorflow.blog/handson-llm/)> Ï±Ö 12Ïû•Ïùò ÏΩîÎìúÎ•º Îã¥Í≥† ÏûàÏäµÎãàÎã§.\n",
    "\n",
    "---\n",
    "\n",
    "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
    "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGw_POflgIII"
   },
   "source": [
    "### [ÏÑ†ÌÉùÏÇ¨Ìï≠] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>ÏóêÏÑú Ìå®ÌÇ§ÏßÄ ÏÑ†ÌÉùÌïòÍ∏∞\n",
    "\n",
    "\n",
    "Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùÑ Íµ¨Í∏Ä ÏΩîÎû©ÏóêÏÑú Ïã§ÌñâÌïúÎã§Î©¥ Îã§Ïùå ÏΩîÎìú ÏÖÄÏùÑ Ïã§ÌñâÌïòÏó¨ Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏóêÏÑú ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄÎ•º  ÏÑ§ÏπòÌïòÏÑ∏Ïöî.\n",
    "\n",
    "---\n",
    "\n",
    "üí° **NOTE**: Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏùò ÏΩîÎìúÎ•º Ïã§ÌñâÌïòÎ†§Î©¥ GPUÎ•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ï¢ãÏäµÎãàÎã§. Íµ¨Í∏Ä ÏΩîÎû©ÏóêÏÑúÎäî **Îü∞ÌÉÄÏûÑ > Îü∞ÌÉÄÏûÑ Ïú†Ìòï Î≥ÄÍ≤Ω > ÌïòÎìúÏõ®Ïñ¥ Í∞ÄÏÜçÍ∏∞ > T4 GPU**Î•º ÏÑ†ÌÉùÌïòÏÑ∏Ïöî.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgSOOCM8t0lS"
   },
   "source": [
    "ÏÇ¨Ïö© Ìå®ÌÇ§ÏßÄ Î≤ÑÏ†Ñ\n",
    "\n",
    "* datasets 4.0.0\n",
    "* transformers 4.57.3\n",
    "* bitsandbytes\n",
    "* trl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UatvUaylSbPZ"
   },
   "source": [
    "*trl Î≤ÑÏ†Ñ 0.17ÏóêÏÑú SFTTrainerÎ•º Ïã§ÌñâÌï† Îïå KeyError: 'completion'Í∞Ä Î∞úÏÉùÌïòÎØÄÎ°ú trl Î≤ÑÏ†ÑÏùÑ 0.16.1Î°ú Í≥†Ï†ïÌïúÎã§.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "H9EuD4pvgIII"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bitsandbytes trl==0.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5luSSUAu_6d"
   },
   "source": [
    "## ÏßÄÎèÑ ÌïôÏäµ ÎØ∏ÏÑ∏ ÌäúÎãù"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPtcbw38_hVi"
   },
   "source": [
    "### Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593,
     "referenced_widgets": [
      "b519d289441b482cabc4d356466b7080",
      "5b56759b6146429d811763b242497deb",
      "365715bf6c4f45d1b0a282927c222b4e",
      "090ffde2233745408276e2c5d99afca1",
      "5931855f365e422883087a1829d2982f",
      "4514d94a81de48ea8f2d60a796ee4d86",
      "cc00955335d94e34b7f4c760f89afbbd",
      "b9446874021d42b78a323089582f9105",
      "5e384e80624b4343914ec12f5e76cf32",
      "43c0de726e1449dd9ec6a041f9141852",
      "f93a5951379f43f98f14254f66362c79",
      "f64e14f2877640149ee4e982a4d5190d",
      "880dcd2e9133460d83d4e3e2c47c9a7f",
      "a349f62ed6cc4427907905ea3a08965a",
      "490f29587a714b7495bbcf339d385375",
      "89fd812ed345430ca1f55872a8ea01fb",
      "0f3f4329aab148b5952d0944231a55bd",
      "d872cd4c0557410eb915967ae0e09ffd",
      "2198195e932e4f4d9237e40604b2554a",
      "858622d2ad1347b59b3640b022347339",
      "60da439b5dee443d9474c0057f2f6e69",
      "55874f7aef7942b29a7888d03c2c7998",
      "0118b09f7de54a00b3518d5165e1cdca",
      "02f4320d72234d88ab3a0c5e1ffacf95",
      "724f9b4e22a74fd5b87a767d93b32f5e",
      "3318821e360d4956a80fee56320621fb",
      "97532417ed504b26a4f4ffe0a900ec3d",
      "5f4989e0aebb4971823e3b41b536e24a",
      "cea826a8eabf46559ebb606dc73b92af",
      "2ac9a940a05440b592a16906693cece8",
      "561e22fac40c4fe38aa7ffcfcc62aa12",
      "a9942d541be9450cb03fbd89db6d63e9",
      "bec908b2dea44f1faac2d7f07f435fb2",
      "48bc81648f644d499b307f692b7b2ae8",
      "4be9634d668f4d6ab3472999c9710730",
      "c3d7b8d3be4e4511a4f03f793ea5287c",
      "0a94af3f133042c8ae8785441b7adf88",
      "7374ebff7f694cef8025b60b12c36b79",
      "2ed9971e77304cc7a4dfe20e7ba1cf6e",
      "2e3c709a8ebd464a8846de759f1bb801",
      "59d5bae2a5514e3fb4aa42b1c166474c",
      "7ef7612819ad43828617574e9d21f8d0",
      "73993c634cd94c51b5980e0c68a539a7",
      "9af47ffc6ac94a1cb03f56864f8c2ad6",
      "5eb227b0f03d4de68fd5d281c91fd689",
      "b57571bbb20e4ac6b72aaa17b165b804",
      "705c8f15dfd14c6c809d570fb1e412d9",
      "3aa7c3ae20c141c0a9acc652c198931f",
      "f8471e7190404b72964ddba0c8b47ffe",
      "7a710beb6d5d4e81b1a208e05a3e1af5",
      "0e58b630d0624ce091db7949421fd500",
      "48fa58854fa44425aebf7442850f3c9c",
      "dad5d445b4794e27831aacb281d29d70",
      "870845cefc944eda99857a66954d6685",
      "7294ea6b2b4348459f95cd2d87ac0ed4",
      "716821a0de26433494cd39d47b33cc3b",
      "344cd45919e04c2086f945f92863524b",
      "4007dbc184db4de4b3145c17c33fb6dc",
      "8626392e56574142b9b917804498505a",
      "b8c3366a32b94c4a881ef23c124a2341",
      "93067bb33c354a3db2b7bd703f391521",
      "096e4f42a4b146f98737da891147e8b7",
      "ef72f36fe5e046aeb46954b0d018b123",
      "75230fbbb9f947da9b42f4bafda3e436",
      "bafba0a73e2f48afb4dd1a3df4bdb0ef",
      "4003d2a2f1cc496cbd41b25a23829137",
      "6a2564ef04064841bcb48d5193137df4",
      "0886019e87fb4836b327f44f7955a116",
      "3b1fd5747768413396a848e398277471",
      "ab1013d8fcf94c908ee06968088d27c4",
      "6b0c27436a4047bcaaddd5538eee2b21",
      "ef39cbb7625d4a5aaadff3a7eb5e372b",
      "3af71d2dc602461198321c0e68ebd72a",
      "637297c14cc54b86acba72256b0ce3a2",
      "a822086e77fe44e3bb902288bcfcdfc7",
      "54291f04c36a493f8552e40d924506c5",
      "bbbe2a4d80bb4b97a98c919befa9674d",
      "915fbd017aa24a1f847b188ce1b2f280",
      "2203d288aa2a43a1a63ca24bec188049",
      "762225c9822e430c8fba0862645971b2",
      "d027e7bdf4b64a69ad20fd39fb07db3c",
      "62bfc08c3af24e308c1bd91b63bce518",
      "17b098673b96411f9e998272a1fecbe2",
      "565073aa10a74367b860eadf82969b09",
      "5f3b0faeb74b4d3abed505ba58585fe9",
      "55de646821ae4d4ead526b0a80d12a7e",
      "20584a11d55e4859a34a31855f01fee2",
      "9c31cbd320944cb5860211cc88a5ed8b",
      "811fd2a45005452ea600721c4026dae6",
      "8a7e6fc1071d4a668b06079a8c996b93",
      "525f783243fe40ff8d99473c491448bd",
      "30bf3ea29b814fbea3b897ed8f5a3798",
      "18b30e5ba2d84028b65370f6bd3012d0",
      "bcbb0129e0de4f80b793ea3c3b481793",
      "0f0fde090a524be2a22fb38ce12db040",
      "0fb967488bfb4578adda283609fd1209",
      "0624122fce3746f1abd1edcdebce0d83",
      "86de6e20a8104e4e8111f3a5283657d6",
      "075789eab80b4ffc8042e729807d3ff3",
      "eebf24a2b82e4280a4ae7b8bf07174ee",
      "ec6a887852aa4be9867b19192c8400b2",
      "c75391e768164ebb8d90a2d759fb78e4",
      "0ce027a059f149dc8e930049d8692569",
      "3e45fec9f1424b56af59788dc3c99154",
      "4b983c26d7374ae2bb018638d2bab365",
      "582d91d400c2442fb11f294805be1ed5",
      "3b2d3aa0dd5e45448213e78bbbce9bc7",
      "fcabb3524a8a477aa417fe34e834a1fe",
      "178a65520b154d72b5055f1eba8af30c",
      "7e6c77f3b5ad46c4808316d4125fd930",
      "ad527b90cf954bcab754ee6c47ca8cdc",
      "2874ec3275b6411c9d0f42af43037eec",
      "fd4ee0a63b704e77acbdfa6fae99a6b8",
      "d0c7a406326f4b38b8e1b3cd1142402b",
      "c7928d772bd64504b31cb08a20ab0773",
      "83e74d46540d4ff1bcd2837a1b39acb0",
      "51497df913044632893c75817233b094",
      "a1152c3f06c84bb8822802c89a0b1155",
      "29d5488e365448d7954df10f243f75ee",
      "9f2532616eef415ea2906bbcaf697245",
      "84f9287df4b44d56856f3ee4ed997fc7",
      "08c9e258f2444dbfbc89a597e962574a",
      "2dc674cdc13f402c8dd3c52b96fca841",
      "8b8393dc37234cfdb906f4beb1bf9b8c",
      "1d868bc2380d4286a796b4a38f77f1af",
      "ea4f6102a2bd466f823e187fe9fadbca",
      "565d24c8469e4a7098c88b16c0cf5f4a",
      "82ffbb2a326e449bbbed6d4f4e0d16ef",
      "75e7de37ea3b4af3b1a6478ff70102c2",
      "4c198689acdd463fac1cb847b7753577",
      "b8cb24dfe75c44dfb444f11ec8016beb",
      "55bae814515f4de1898f0c12623946ab",
      "bbd1375245974cf187f543a7d29e9be0",
      "a7e65a3778054bf2818ae0a46df044dd",
      "0643ea2635f5404db2e071323535ca3f",
      "a12025adb8cf4dc885b18bd4e35ab7c2",
      "4dbcc277836741ceb3d55ca439fb4003",
      "83bb47723bb14124b20ae8c6e77e2164",
      "4566c38cd41a4f259a9eda0b011df667",
      "ea6942214cb344c3aa5259cab3653f4e",
      "8c95837e08af43048109507f2d2512a3",
      "d2e026559ad14cf38ac15fcb597486c6",
      "65b3dfdf9e0047a0820db938770b8bcd",
      "2bc61408126c4494a37cd4a5e3c799d9",
      "231cdc7e4f5c4381acec71b058461a90",
      "a5f933cdb12747e280fc6ddc61273eb2",
      "008c5a5481c843f79b3c12e4a186e6da",
      "72769c7bd0c34f26a5e6ec0a5f2e3768",
      "7db26aa6950645eca1f13ba8f774e3e0",
      "0b9dd1e008a04cdfa4ad786caec9d1d8",
      "4f7511ea987342d7a808a69bb436b111",
      "cc6f8a292648418aad7f84b47b6e7451",
      "b4cecb2e32c945ff8c16a25915057df1",
      "5fafb1f8c3d64880910959d34d398621",
      "852e3aa4c9cd4cc39dac4075933b3a05",
      "12fe43b0b30941cc8a6f046691b19d51",
      "ce274d74903944748cd390cc2df1804e",
      "8ceaf0ec995648fea7614164af5844f5",
      "0017a5de56f648dfa97cd2a9a098cf52",
      "9eaa15414ee8478b820453dd318a5ba1",
      "b471f83fb60f4eb084ddeecef70640a6",
      "bd111bf2845b4ee6aec72b54c8e0491b",
      "336752e80c784f3093959156a2125bf2",
      "d161d785281a4022a834088e3e41d57d",
      "00c29a07fa1543e0b9a2118be13771dc",
      "8af7d177dc0f4e86b8d9f38e5892c49f",
      "b8ece05fe0194adea729566ed560f175",
      "d0cf9295e1184723bd73380726672e09",
      "04ad548f5a8c4f43987b243ef4182038",
      "9b513cae62bf45d7a2f7cf0f40d5ab52",
      "1550fd728267418892f3f0f2a18c69c6",
      "424ee7460a034268bc090864d7c7fa31",
      "839e3a396b6c4fcaa6a03ddb7fda8120",
      "d4983747ed924a5887df508cb1a3f78d",
      "049eb62f84ec4c94888d62c209626577",
      "fa6a9b7e044e430a9e68d36d2e201863",
      "6329280f2b53474f96783480c4111396",
      "3c8c8f1cea904940adb4cb04a85bf40a",
      "52b8feecf4c44da8862f79afa90bbfbc",
      "5eb3d8524d014666abcc6345c7beb656",
      "1b4080cde00244a182398472adc705e4",
      "43220b5855f241cc86ed523a52e07ca2",
      "21cc8b4147ad49dc921394a37370975c",
      "d4ae16e140e94c94b0a9da54a279c133",
      "d3d8ef1b5605455bbf9b42d1c027e0f5",
      "57f3d41fe9b643068cf548f7c4701f32",
      "56b83ca34087401fb3496b214f3e17f0",
      "fb9da609e22346939ea2607fc90adeff",
      "35c3cf8b09a84bf98e851cc2ed9081b0",
      "9132cc8e14fd471db9e2e1bfe5b76985",
      "6b2c03b8e2614d6ebc8fc78d1cb55356",
      "72780228b9504600946ef1bf7edfc762",
      "65de51270b98494b9f89582938b7394f",
      "07f88589f5c94751b3a75d869e12bfc3",
      "14fa6a07018d46179c6913b4192a873e",
      "d413ada55b4843c4a0d99d96fa884d6c",
      "1c063a2f1e514acb8f1a0d5d6ccf7c1d",
      "7e067d76f7db4dcda4e133461505b7b5"
     ]
    },
    "id": "SqeZchJiOXdd",
    "outputId": "1523cf21-0790-4c1f-8aae-c47475768c58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b519d289441b482cabc4d356466b7080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64e14f2877640149ee4e982a4d5190d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0118b09f7de54a00b3518d5165e1cdca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bc81648f644d499b307f692b7b2ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb227b0f03d4de68fd5d281c91fd689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716821a0de26433494cd39d47b33cc3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00000-of-00003-a3ecf92756(‚Ä¶):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2564ef04064841bcb48d5193137df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00001-of-00003-0a1804bcb6(‚Ä¶):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915fbd017aa24a1f847b188ce1b2f280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_sft-00002-of-00003-ee46ed25cf(‚Ä¶):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "811fd2a45005452ea600721c4026dae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_sft-00000-of-00001-f7dfac4afe5(‚Ä¶):   0%|          | 0.00/81.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebf24a2b82e4280a4ae7b8bf07174ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00000-of-00003-a6c9fb894b(‚Ä¶):   0%|          | 0.00/244M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad527b90cf954bcab754ee6c47ca8cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00001-of-00003-d6a0402e41(‚Ä¶):   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08c9e258f2444dbfbc89a597e962574a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train_gen-00002-of-00003-c0db75b92a(‚Ä¶):   0%|          | 0.00/243M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd1375245974cf187f543a7d29e9be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test_gen-00000-of-00001-3d4cd830914(‚Ä¶):   0%|          | 0.00/80.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc61408126c4494a37cd4a5e3c799d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_sft split:   0%|          | 0/207865 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852e3aa4c9cd4cc39dac4075933b3a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_sft split:   0%|          | 0/23110 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af7d177dc0f4e86b8d9f38e5892c49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train_gen split:   0%|          | 0/256032 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6329280f2b53474f96783480c4111396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_gen split:   0%|          | 0/28304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9da609e22346939ea2607fc90adeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Ï±ÑÌåÖ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Î°úÎìúÌï©ÎãàÎã§.\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"TinyLlamaÏùò <|user|> ÌÖúÌîåÎ¶øÏúºÎ°ú ÌîÑÎ°¨ÌîÑÌä∏Î•º Ìè¨Îß∑ÌåÖÌï©ÎãàÎã§\"\"\"\n",
    "\n",
    "    # Ï±ÑÌåÖ ÌÖúÌîåÎ¶ø Íµ¨ÏÑ±\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† TinyLlama ÌÖúÌîåÎ¶øÏùÑ Ï†ÅÏö©Ìï©ÎãàÎã§.\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n",
    "      .shuffle(seed=42)\n",
    "      .select(range(3_000))\n",
    ")\n",
    "dataset = dataset.map(format_prompt).remove_columns(['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dtl2xZptgyDf",
    "outputId": "87aee9e3-3077-4067-ecc2-da21c7decf89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Given the text: Knock, knock. Who‚Äôs there? Hike.\n",
      "Can you continue the joke based on the given text material \"Knock, knock. Who‚Äôs there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>\n",
      "<|user|>\n",
      "Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ÌîÑÎ°¨ÌîÑÌä∏ ÏòàÏãú\n",
    "print(dataset[\"text\"][2576])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyuLZGizDqUB"
   },
   "source": [
    "### Î™®Îç∏ ÏñëÏûêÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "eb8b33344b204a13a64eb9569b7deb68",
      "820fc31db9724ef6aa224cbdffc53006",
      "b61badde303d4b1ca007a73fa8c92f23",
      "d171b5e7e5fb4ee087f644806a9c6308",
      "b5c2bdc5a98548f8a902c6370d420d4a",
      "c25b1dfb393643a4a99e574889f72169",
      "5615d177fa3844dabf88e5164acc1b0c",
      "9539ab50c7f947618c257a5cc93d9982",
      "979e40e1c0ec4d818a2ad9946ace007f",
      "4d166885d15a4495bebbd94fd3f1bf66",
      "da3266e94a0a47ff88203144a5592284",
      "530e6aa1fd7849028386a4e5c52f9345",
      "9dd36288f3a543e5a2aec27f0bbe8ccf",
      "c25a60bc993b4af0bf59a2b411bf60d3",
      "c0e07215d6f44c1fb35c16f6f38a0081",
      "a21233d7e91b4f0594a2627b1e3709b5",
      "4af3adfb9f2e4e5a964cc9b7ede48975",
      "ad641df955184bf8979166297a7f7033",
      "113def2a44fa4c6dac6dfeb422fc16c4",
      "ed3be43ba5164a378684ce6d5701122a",
      "10fbabf66eed4a25adc21002c5ccdccf",
      "dcee3cb61571472ea704cc97acaead86",
      "3189fc29456346ec9b8a19d0297aac24",
      "ab87637da42b4663999969d024920682",
      "a592a9efb6c040ea8183dc5f8f3391d1",
      "e300962082ea46e2ba7b6696ec84602f",
      "3c35bfc72d784cd7b4ac2c1525f677a3",
      "f93fc6206e5b48f39d4a174a2d395212",
      "8ca3add2547447e18a95ac20ac6fc23e",
      "33c8bb9ef5e849008ea9541e1d24f855",
      "3e904a830fd44c6ca664b208364ced02",
      "2f5350b8c99a424785864dde7336fc55",
      "b4723a45be2146549918bb6cf20931ee",
      "174c54216bd5425d9580ce176bae9f85",
      "3a492c2ca5e740949ada440c827ad4b9",
      "b2e6edcb5ef142d5ab172ccdfc209367",
      "e289d732a5764b41ab144329108e4d25",
      "43883f22c8fa4537b1a149ee4e732739",
      "94e786e872b6403bb7a047cf77868c22",
      "1664d96742b240d0ac6ed590a46deea9",
      "c10c5038c5584d4ea52e52dd6835c6c3",
      "fb1423450fb64015b5efa3768b42cbcf",
      "ef5f3f207d7e46f389ccacf8f6d06b9d",
      "186d8e5d78544f6dbf509f42c3a41936",
      "f7b9f52ca7de436e8a9db8bbf85aa110",
      "bff3a5c3f9a041fab9dda1474a0d218c",
      "26aceebe497d457cb0dd7a87368cec7c",
      "c4c69a6f14ee40be81e44527f59fc220",
      "04c6b593de3849e8a08c2e54d73fa746",
      "8c421e9031ea43cb852d48008370ec75",
      "cb46979ba5444f94aeefdf637534abb5",
      "8c4e02d955fd48808c6748dec1e55a37",
      "128a37fa362f464ab0d9c82463625a19",
      "9a21c1b378be45e996fbe0bef004c540",
      "1e0cb2c7106f4cb1bb7c03d1dddcf2ca",
      "ffb82b4572f8489f9be07e86d0c46e6b",
      "4c0bde5b35de4c9594e153fe93ed547d",
      "351a94b6b34643b0aa87b5b9d68d0ff0",
      "de4c51c6f3b242c18e3affe3b8aa26a5",
      "e8852b0e74a54f0680ddb4a66dac08eb",
      "7e928804e352456da484815dece48e3a",
      "7562b20866c64bbab367ac8a477ff749",
      "fd8ba6e4e22c4fa8b1b8e93595ccdf77",
      "7fd68ba3b3034bc093291f19458bcd64",
      "6fe8175b1a39459ca47e01550d9fde54",
      "0812b5b1980f4451b41feb843446f33c",
      "0a6007fb5f4c4eaeb6d5cf3ddb96d5d1",
      "a8dfef941b374207b70ec18781b15f6d",
      "40114256fcce479fb1b23fe9e64e4cfb",
      "23af04ea74214fd19e65bd6d58ae0972",
      "9ddb4cc15d634b4394d89963475bea58",
      "39c7700156d64b5c9d3be6f050beba03",
      "8ed53657c6d5455b98d0b5ac681e099c",
      "84264ad5137844fab81052a58c36e1a2",
      "491ead443d5b4deb9b35cf44439ae1a3",
      "877672f6e2b1429284c3c1c7e123e780",
      "a7db8169d5f242f888fef91c8e5f9044"
     ]
    },
    "id": "M95Y207T7wSp",
    "outputId": "4fce9dd3-26a5-440e-9c89-739cf3c0db65"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8b33344b204a13a64eb9569b7deb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530e6aa1fd7849028386a4e5c52f9345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3189fc29456346ec9b8a19d0297aac24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174c54216bd5425d9580ce176bae9f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b9f52ca7de436e8a9db8bbf85aa110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb82b4572f8489f9be07e86d0c46e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6007fb5f4c4eaeb6d5cf3ddb96d5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-ÎπÑÌä∏ ÏñëÏûêÌôî ÏÑ§Ï†ï - QLoRAÏùò Q Îã®Í≥Ñ\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-ÎπÑÌä∏ Ï†ïÎ∞ÄÎèÑ Î™®Îç∏ Î°úÎìú\n",
    "    bnb_4bit_quant_type=\"nf4\",  # ÏñëÏûêÌôî Ï¢ÖÎ•ò\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Í≥ÑÏÇ∞ dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Ïù¥Ï§ë ÏñëÏûêÌôî Ï†ÅÏö©\n",
    ")\n",
    "\n",
    "# Î™®Îç∏ÏùÑ Î°úÎìúÌïòÍ≥† GPUÏóêÏÑú ÌõàÎ†®Ìï©ÎãàÎã§.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # ÏùºÎ∞òÏ†ÅÏù∏ SFTÏóêÏÑúÎäî Îã§ÏùåÏùÑ ÏÇ≠Ï†úÌïòÏÑ∏Ïöî.\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# LLaMA ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1iGIch-sAMC"
   },
   "source": [
    "### ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86o1T5n4DziD"
   },
   "source": [
    "#### LoRA ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0tYs1ZhYDyw9"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA ÏÑ§Ï†ï\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,  # LoRA Ïä§ÏºÄÏùºÎßÅ\n",
    "    lora_dropout=0.1,  # LoRA Ï∏µÏùò ÎìúÎ°≠ÏïÑÏõÉ\n",
    "    r=64,  # Îû≠ÌÅ¨\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # ÎåÄÏÉÅ Ï∏µ\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# ÌõàÎ†®ÏùÑ ÏúÑÌïú Î™®Îç∏ Ï§ÄÎπÑ\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhbh7kKuD24o"
   },
   "source": [
    "#### ÌõàÎ†® ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TwxZkx80G6bO"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# ÌõàÎ†® Îß§Í∞úÎ≥ÄÏàò\n",
    "training_arguments = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtwIo5a0D6f1"
   },
   "source": [
    "### ÌõàÎ†®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "89ab35c67f6f444eb6827ec1f4824dc1",
      "8150c4d4f251427da46d81aef4ee4e68",
      "f338911e1bda4db599944f32e015d730",
      "2867eb2e94b34dbb9c67c5c719732b4d",
      "adc76182f0b14194b7480f372624fc51",
      "6b8aee4a85e54b57ba40cc4c31a602e5",
      "acfb69f903194871a16210c84279a8a6",
      "ce13d059192b4da4a06123b20ecba016",
      "7544f35f018540abbfb34ee78c5fdb98",
      "626551adf438477d86d58ae420e30a51",
      "7f11c59be2db4b07b5706441376abfcf",
      "4d81e2f1b0e64137aaa9ebcc1bfe1134",
      "e50ebc21377e4e88a42b3efb52d3bbcd",
      "f0a3efaeac6f41d8850fb0d4b5d7b98a",
      "40fbad65e3e8454199d970fc108bd2ad",
      "21e07bc0771141b9b8b176b759511759",
      "a827a51fd1bd4aa5803a0a3f80afff64",
      "4cdf11b750b145dab63f048637c9a310",
      "2c9ebc2a5a8540b7af52ea1c1b3d0591",
      "2e45809bc62b4f7593ab116ccd7b79f3",
      "ff26a32174b34cddaaa05086836753e0",
      "a270a8a018b1412b8a3054358816836c",
      "4e80a39792874ac393b94461193056e9",
      "32063a74538946f38e686284ead4b27c",
      "15b2565a680844b8b7e5416e8875df7b",
      "4628cd9fb3d54947ad5f1e32fba7ab9d",
      "ac3c6f37d82a4a8293bdcb827aa2635a",
      "7359ce5128494c74afba0e159dd05e9a",
      "fb712d77ab6c4500800d1b819f92b208",
      "392f3fd1c7d74a48bf97a74744e49e45",
      "8f2f2dbfa5ae4be4bf601293d40b0e2c",
      "97440d6db1354b3f9eee1f21f11dad12",
      "21041fc3342a43bf83a1c80c25bb363b",
      "9ef9b7f4a4b74d0bbe2fb627b495f39b",
      "7072b7fd33344bf69e0968c71ef6e6ef",
      "fec5870a7b59467c87b5b0f1234b79eb",
      "48cdbd5526eb48a9a99b637b0ffa283c",
      "14f85433e3684a509c26d581dd4e6313",
      "3267d64dd84a437995c8c3412874b479",
      "71df7574b7f04a74bb91b8a24c5cc7ce",
      "9ae395e63feb42e7a6a2423cd9c307c1",
      "28a9162626b6414a8c031351f3b7d7b3",
      "9f7dc900eb384a7facd0558acb22a4f7",
      "d10978571caf47b88ef37d6ee97035df"
     ]
    },
    "id": "B2D7RVihsE7Z",
    "outputId": "2f3732a2-cc7a-4eb6-bd6d-1033de2bfe66"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ab35c67f6f444eb6827ec1f4824dc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d81e2f1b0e64137aaa9ebcc1bfe1134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e80a39792874ac393b94461193056e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef9b7f4a4b74d0bbe2fb627b495f39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing.<br>Run data is saved locally in <code>/content/wandb/offline-run-20260104_063918-e60ufwmk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 10:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.578800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.481000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.366700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.401700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.454500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.374100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.370700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.300100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.430200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "# ÏßÄÎèÑ ÎØ∏ÏÑ∏ ÌäúÎãù Îß§Í∞úÎ≥ÄÏàò ÏßÄÏ†ï\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    "\n",
    "    # ÏùºÎ∞òÏ†ÅÏù∏ SFTÏóêÏÑúÎäî Îã§ÏùåÏùÑ ÏÇ≠Ï†úÌïòÏÑ∏Ïöî.\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Î™®Îç∏ ÌõàÎ†®\n",
    "trainer.train()\n",
    "\n",
    "# QLoRA Í∞ÄÏ§ëÏπò Ï†ÄÏû•\n",
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsIBfv1PsId-"
   },
   "source": [
    "### Ïñ¥ÎåëÌÑ∞ Î≥ëÌï©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M6cPdde4Z-ks"
   },
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# LoRAÏôÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Î≥ëÌï©Ìï©ÎãàÎã§.\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPRYGimIsM2-"
   },
   "source": [
    "### Ï∂îÎ°†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15dJC3ZrdVnK",
    "outputId": "a238e220-eb9d-473a-fced-22c86c5391e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models are a type of artificial intelligence (AI) that can learn to understand and generate text based on large amounts of data. They are often used for text-based tasks such as machine translation, chatbots, and natural language processing. Large Language Models can analyze text data and generate new sentences or words from a given set of input words, which can then be used in various applications such as chatbots, machine translation, or natural language processing.\n",
      "\n",
      "One of the most significant benefits of large language models is their ability to generate long-form text. By analyzing large amounts of text data, LLMs can generate complex and intricate sentences that are difficult to generate by humans. This ability to generate long-form text has led to the development of applications such as text generation platforms, chatbots, and natural language processing engines.\n",
      "\n",
      "Another significant benefit of LLMs is their ability to understand context. By analyzing the context around each word or phrase, LLMs can understand the context of a sentence and generate a more comprehensive and informative response. This can be helpful in scenarios where you want to provide context to a given piece of text, such as in customer support or educational contexts.\n",
      "\n",
      "Another important feature of large language models is their ability to scale. LLMs can be trained on massive amounts of text data, making them ideal for applications that require high-quality and context-aware text generation. This has led to the development of new applications such as chatbots, machine translation, and natural language processing engines that rely on large language models.\n",
      "\n",
      "Overall, large language models continue to be a key component of AI and are playing a significant role in enabling new applications, such as chatbots, machine translation, and natural language processing.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ÏÇ¨Ï†ÑÏóê Ï†ïÏùòÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Ïù∏Ïä§Ìä∏Îü≠ÏÖò ÌäúÎãùÎêú Î™®Îç∏ÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JNfYZe9vCb8"
   },
   "source": [
    "## ÏÑ†Ìò∏ÎèÑ ÌäúÎãù (PPO/DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar2h9kZ9qmEG"
   },
   "source": [
    "## Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253,
     "referenced_widgets": [
      "3e4af957865246568d1a4c233d05c2e9",
      "fd5cc6b5c5634e8fa2ee393e29806ed2",
      "360ddaaea8f8444aa2a87c0bd6a4e681",
      "d09ec4ddffca40ab9616d363f5e0caa2",
      "ee4d0a41a99e4a74b59c08745fadc62b",
      "eb2180034618469c9d679f330dfa9c63",
      "6b281f54c41e435fa58a4def2d1e4ebb",
      "b5e3f5ebd752429c93502029dbef4e1b",
      "d3143116ac2a43598420a4d565c0906d",
      "9dc7b107a14347fda28016776f5b3634",
      "0172ed47fc6a4e569e2a1d135dd7cccc",
      "57d1c06226bc46b3b677a4cd77b12c00",
      "ee0f80589b444bfb8a12fa7d48e98577",
      "28e221c1fc3241eba413af246b075e1f",
      "bc8ad8aa75d44be788739b1589aa07f8",
      "f64cb30b21fb4e6eb319ddf591427b2c",
      "3fb776b8004b4bf7a7657350d8f19e8b",
      "83b0ca938e94454886d2147f5f5a3612",
      "0552ced1d5f54ba2913e78f739e1c908",
      "7012d784cee0479ba4409db07853ac4c",
      "626f4a040b254179b004f9249ac87dea",
      "e625a3ad28374055ad24f3002e7d88b8",
      "61822ffe33ff4aafab56ab08346288db",
      "121339f6c9684445967931c41ae39df3",
      "8ecbc8a038c44bf3a194ca83d1cc3f43",
      "45214aa959044c4a8050435d861e9f3c",
      "9bf67a68cae442618e979b9feaede32a",
      "7da4cc76f5e74750942c0dc9f71c2133",
      "a3fd003980114d479ea3917e55051d7f",
      "1886632d28514ae9ad15fb830b280986",
      "b1cc0d22d2fe451bb18b428fcbe217e1",
      "f6f5090f6c3246799f1f16b0aeb6afc1",
      "e8f1520a9db04ae18e32d224a3712789",
      "cca5a08f769b4cf6bdc2e486fa539a98",
      "ce1552597c26495bad6c33dc3d7e27ef",
      "10083b178de3401abdc213b8ea2d82e9",
      "f242a9cc5d4a4e778ca7d967e28ac200",
      "577d2e038fb843a8aa986224567d20ce",
      "8aa9c50e9b964d55a003808b19399fa7",
      "5914191e7ece440093f93456f5e1b1d7",
      "8509eee965d0478f8a0862c6c4c6bd0e",
      "3821d0e39a4d4b80b06fe0b40e639dc8",
      "ad673ea0ee8c4a79b3ab48c4480dc02e",
      "e7cb3a16f4ed4a6cafb555d8e90dc5ab",
      "deedfc06b8ec4cc8b8dd26a9041d6cc5",
      "ab213fe4961347109f79b75e3308f7f1",
      "a30138a5c6d14b3ca941679db8a79a8f",
      "683ad7951cb24e0a9fe93d051ef6d072",
      "922f0720757d4a799d4ba39b9ec9309e",
      "100ef1869aeb47808083cca685a43a3d",
      "b66159ec4e6146fc8739e690f9a2bc05",
      "15bcf1e5396049f58d2e375d1b975696",
      "3c98380464514ea0a3eeaeaa7e9d00d5",
      "d31a831271af4d3682999a15171ffad4",
      "eabeeaf311434c22a640d30dd1f21617"
     ]
    },
    "id": "UlbPVO_aac33",
    "outputId": "28c9fd78-7422-461b-db93-74ce0c280710"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e4af957865246568d1a4c233d05c2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d1c06226bc46b3b677a4cd77b12c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/79.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61822ffe33ff4aafab56ab08346288db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca5a08f769b4cf6bdc2e486fa539a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12859 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deedfc06b8ec4cc8b8dd26a9041d6cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 5922\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"TinyLlamaÏùò <|user|> ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©Ìï¥ ÌîÑÎ°¨ÌîÑÌä∏Î•º Íµ¨ÏÑ±Ìï©ÎãàÎã§\"\"\"\n",
    "\n",
    "    # ÌÖúÌîåÎ¶ø Ìè¨Îß∑ÌåÖ\n",
    "    system = \"<|system|>\\n\" + example['system'] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example['input'] + \"</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example['chosen'] + \"</s>\\n\"\n",
    "    rejected = example['rejected'] + \"</s>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖãÏóê ÌÖúÌîåÎ¶øÏùÑ Ï†ÅÏö©ÌïòÍ≥† ÎπÑÍµêÏ†Å ÏßßÏùÄ ÎåÄÎãµÏùÑ ÏÑ†ÌÉùÌï©ÎãàÎã§\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r:\n",
    "        r[\"status\"] != \"tie\" and\n",
    "        r[\"chosen_score\"] >= 8 and\n",
    "        not r[\"in_gsm8k_train\"]\n",
    ")\n",
    "dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkCJ4CO5sQG6"
   },
   "source": [
    "### Î™®Îç∏ ÏñëÏûêÌôî"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YMmilm7c1-P",
    "outputId": "10f50b97-5f77-442f-eefa-52c5c7b7429d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# 4-ÎπÑÌä∏ ÏñëÏûêÌôî ÏÑ§Ï†ï - QLoRAÏùò Q Îã®Í≥Ñ\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4-ÎπÑÌä∏ Ï†ïÎ∞ÄÎèÑ Î™®Îç∏ Î°úÎìú\n",
    "    bnb_4bit_quant_type=\"nf4\",  # ÏñëÏûêÌôî Ï¢ÖÎ•ò\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Í≥ÑÏÇ∞ dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Ïù¥Ï§ë ÏñëÏûêÌôî Ï†ÅÏö©\n",
    ")\n",
    "\n",
    "# LoRAÏôÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Ìï©Ïπ©ÎãàÎã§.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# LLaMA ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º Î°úÎìúÌï©ÎãàÎã§.\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iidCbaXMs1O4"
   },
   "source": [
    "### ÏÑ§Ï†ï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6IfkvLkylVD",
    "outputId": "c77302b8-06db-473c-c894-a55513b8dd12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# LoRA ÏÑ§Ï†ï\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=128,  # LoRA Ïä§ÏºÄÏùºÎßÅ\n",
    "    lora_dropout=0.1,  # LoRA Ï∏µÏùò ÎìúÎ°≠ÏïÑÏõÉ\n",
    "    r=64,  # Îû≠ÌÅ¨\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # ÎåÄÏÉÅ Ï∏µ\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# ÌõàÎ†®ÏùÑ ÏúÑÌï¥ Î™®Îç∏ÏùÑ Ï§ÄÎπÑÌï©ÎãàÎã§.\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lk-cEEd8nk27"
   },
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# ÌõàÎ†® Îß§Í∞úÎ≥ÄÏàò\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 997,
     "referenced_widgets": [
      "61fd774e7cf047a5a2f06b40810185a2",
      "3997506844a54ea5854bf7e21ff8f187",
      "876105adee6545b7a9bbe9da1dcae875",
      "0e5518d9e7504db7a7c91f68570a5e22",
      "6fb7ee6762d3415197045e6dd6262594",
      "cd7ed5242210429998399bd3028e1562",
      "116d04c7607641a1a0f2c073d4d2907e",
      "a1f9d36607664ddd993942dcc2f76258",
      "e9138864172c4644bc3f781bb07dcdb4",
      "abcf28dafe2c48028bd2fb41bb0b7631",
      "3f0c04bea85d458184bde9e57a3a1bfe",
      "8c88718f71104ab7998fd2c54a8fa771",
      "04d339acd06e40ac8a3106eeb40db7d9",
      "126d57fc815a44d6826cfe1d294f609e",
      "42fcd0cfead54824adf9106426b20d24",
      "3f5aede257e545b4a6d1b1a3ee4ce42f",
      "f29df20947ce4ff683b852c9b1c54a67",
      "538e4e2d34b24b639c7f066b872a2261",
      "272aa16b77be444b84e88425f656b0fc",
      "df9d2e0c0691460da5de217ffc5d583f",
      "df19f21c109b4e01a08ad0976554921f",
      "62f9a312576247f3b087d1885f33729d",
      "d7b5d811e73c40df9f4ef56f4116ad4a",
      "a464cf3c03c441329706c1c1465b81b1",
      "59ff6bf9a6d745668accb98d680c1236",
      "a21440d338584f78a4a9effcf48184c0",
      "f5770858b1b148398c77c4ed5d1e436a",
      "252a42f299b14a4cb7391b4547f89344",
      "a8eed8628bb1490191ab6b5657bf1ec8",
      "73aafe3917574341af843ac8ddc4966b",
      "9eab53c897344d4ebc34b5d45c40ddda",
      "6ecf724677564a899eed295f8b7b61ac",
      "535a9421867b48a2aa1c997ee92336f1"
     ]
    },
    "id": "Pp3tUXhWm0pE",
    "outputId": "6b3c23cd-2479-45c0-822d-79c0bc155ebb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61fd774e7cf047a5a2f06b40810185a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c88718f71104ab7998fd2c54a8fa771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b5d811e73c40df9f4ef56f4116ad4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 07:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.649700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.552000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.553100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.501500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.369800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.507900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.493200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.462000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "# DPOTrainer Í∞ùÏ≤¥Î•º ÎßåÎì≠ÎãàÎã§.\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dpo_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "\n",
    "# DPOÎ°ú Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ ÌäúÎãùÌï©ÎãàÎã§.\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Ïñ¥ÎåëÌÑ∞Î•º Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QFE4OKFvyLMe",
    "outputId": "d63cfa71-61c5-4271-cb5a-3e3efdab15de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/peft/peft_model.py:598: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# LoRAÏôÄ Î≤†Ïù¥Ïä§ Î™®Îç∏ÏùÑ Ìï©Ïπ©ÎãàÎã§.\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sft_model = model.merge_and_unload()\n",
    "\n",
    "# DPO LoRAÏôÄ SFT Î™®Îç∏ÏùÑ Ìï©Ïπ©ÎãàÎã§.\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    sft_model,\n",
    "    \"TinyLlama-1.1B-dpo-qlora\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "dpo_model = dpo_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAkwJcHYmxr4",
    "outputId": "c2c95307-2ae6-4a25-a59a-8b569c69ab83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are an important component of modern machine learning systems that can be used to generate natural language. They are a type of artificial intelligence (AI) that learns to translate textual data from one language to another. LLMs are built using deep neural networks, which are complex algorithms that learn from millions of examples to produce high-quality outputs.\n",
      "\n",
      "LLMs have several advantages over traditional machine translation engines:\n",
      "\n",
      "1. They can process large amounts of text quickly and accurately.\n",
      "\n",
      "2. They can generate text in different languages without the need for additional translation tools or resources.\n",
      "\n",
      "3. They can produce high-quality translations that are consistent with the original text and are frequently used by machines for natural language processing applications.\n",
      "\n",
      "4. They can be customized to specific languages and domains, making them particularly useful in areas such as medical translation, where accurate translations are critical.\n",
      "\n",
      "5. They can be easily integrated into existing machine learning systems, making them a powerful tool for automation and efficient translation workflows.\n",
      "\n",
      "Overall, LLMs offer a significant advantage over traditional machine translation engines in terms of speed, accuracy, and portability. They can be used in a variety of applications, ranging from translation to natural language processing and machine learning.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# ÏÇ¨Ï†ÑÏóê Ï†ïÏùòÎêú ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶øÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Ïù∏Ïä§Ìä∏Îü≠ÏÖò ÌäúÎãùÎêú Î™®Îç∏ÏùÑ Ïã§ÌñâÌï©ÎãàÎã§.\n",
    "pipe = pipeline(task=\"text-generation\", model=dpo_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
