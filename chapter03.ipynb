{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adFzzFsB-Ofl"
      },
      "source": [
        "<h1>3ì¥ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ìì„¸íˆ ì‚´í´ ë³´ê¸°</h1>\n",
        "<i>ìƒì„± LLMì„ ìœ„í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ íƒí—˜í•˜ê¸°</i>\n",
        "\n",
        "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter03.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ <[í•¸ì¦ˆì˜¨ LLM](https://tensorflow.blog/handson-llm/)> ì±… 3ì¥ì˜ ì½”ë“œë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
        "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5dPfvShKZ4g"
      },
      "source": [
        "---\n",
        "\n",
        "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‚¬ìš© íŒ¨í‚¤ì§€ ë²„ì „\n",
        "* transformers 4.57.3"
      ],
      "metadata": {
        "id": "78YkTOy0LSXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "8lVE_3idzVyF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_23Z_do-faF"
      },
      "source": [
        "# LLM ë¡œë“œí•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5RLd6dI-Ytm",
        "outputId": "5af63cb6-97f1-4bc8-99c2-141fc995e10b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    dtype=\"auto\"\n",
        ")\n",
        "\n",
        "# Create a pipeline\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REqcz-ID_XgV"
      },
      "source": [
        "# í›ˆë ¨ëœ íŠ¸ëœìŠ¤í¬ë¨¸ LLMì˜ ì…ë ¥ê³¼ ì¶œë ¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17h6TPHluJ-i",
        "outputId": "613c9340-827f-4254-9670-79a8493b50f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Mention the steps you're taking to prevent it in the future.\n",
            "\n",
            "Dear Sarah,\n",
            "\n",
            "I hope this message finds you well. I am writing to express my sincerest apologies for the unfortunate incident that occurred\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "output = generator(prompt)\n",
        "\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoFkdTd6_g5o",
        "outputId": "14fdf237-2cf0-4222-f41a-894ac7bc666f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phi3ForCausalLM(\n",
            "  (model): Phi3Model(\n",
            "    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x Phi3DecoderLayer(\n",
            "        (self_attn): Phi3Attention(\n",
            "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
            "          (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
            "        )\n",
            "        (mlp): Phi3MLP(\n",
            "          (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
            "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
            "          (activation_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "        (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "        (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
            "    (rotary_emb): Phi3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTrwzB67BYVY"
      },
      "source": [
        "# í™•ë¥  ë¶„í¬ë¡œë¶€í„° í•˜ë‚˜ì˜ í† í° ì„ íƒí•˜ê¸°(ìƒ˜í”Œë§/ë””ì½”ë”©)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sEcxYgJxBYbJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"The capital of France is\"\n",
        "\n",
        "# ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# ì…ë ¥ í† í°ì„ GPUì— ë°°ì¹˜í•©ë‹ˆë‹¤.\n",
        "input_ids = input_ids.to(\"cuda\")\n",
        "\n",
        "# lm_head ì•ì— ìˆëŠ” modelì˜ ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "model_output = model.model(input_ids)\n",
        "\n",
        "# lm_headì˜ ì¶œë ¥ì„ ì–»ìŠµë‹ˆë‹¤.\n",
        "lm_head_output = model.lm_head(model_output[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "68YUSS4GBf9Q",
        "outputId": "8a98b320-fbe7-4a1d-b1f1-7c3d5d5b520f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Paris'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "token_id = lm_head_output[0,-1].argmax(-1)\n",
        "tokenizer.decode(token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWWrfC5oBjwp",
        "outputId": "ba68496c-d96f-4b18-de21-8500c57730b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 3072])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model_output[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nC1PdOnTBnxZ",
        "outputId": "a9c18112-de39-4eea-c13f-5051d5d6974d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 32064])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "lm_head_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2_rP4QBqrZ"
      },
      "source": [
        "# í‚¤ì™€ ê°’ì„ ìºì‹±í•˜ì—¬ ìƒì„± ì†ë„ ë†’ì´ê¸°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "B0n6JhNHBrin"
      },
      "outputs": [],
      "source": [
        "prompt = \"Write a very long email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\"\n",
        "\n",
        "# ì…ë ¥ í”„ë¡¬í”„íŠ¸ë¥¼ í† í°í™”í•©ë‹ˆë‹¤.\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "input_ids = input_ids.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwIvt6jSByAF",
        "outputId": "ef4e9d1a-300d-4470-a320-067dc2d11603",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.47 s Â± 218 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFb1dcvJByCW",
        "outputId": "4a01574d-f429-49af-c1f9-149840b8c43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30.7 s Â± 144 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "%%timeit -n 1\n",
        "# í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "generation_output = model.generate(\n",
        "  input_ids=input_ids,\n",
        "  max_new_tokens=100,\n",
        "  use_cache=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}