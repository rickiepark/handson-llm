{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ETtu9CvVMDR"
      },
      "source": [
        "<h1>7ì¥ ê³ ê¸‰ í…ìŠ¤íŠ¸ ìƒì„± ê¸°ìˆ ê³¼ ë„êµ¬</h1>\n",
        "<i>í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ ë„˜ì–´ì„œ</i>\n",
        "\n",
        "<a href=\"https://github.com/rickiepark/handson-llm\"><img src=\"https://img.shields.io/badge/GitHub%20Repository-black?logo=github\"></a>\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/rickiepark/handson-llm/blob/main/chapter07.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì€ <[í•¸ì¦ˆì˜¨ LLM](https://tensorflow.blog/handson-llm/)> ì±… 7ì¥ì˜ ì½”ë“œë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "---\n",
        "\n",
        "<a href=\"https://tensorflow.blog/handson-llm/\">\n",
        "<img src=\"https://tensorflow.blog/wp-content/uploads/2025/05/ed95b8eca688ec98a8_llm.jpg\" width=\"350\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtUx27GOCAYd"
      },
      "source": [
        "### [ì„ íƒì‚¬í•­] - <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>ì—ì„œ íŒ¨í‚¤ì§€ ì„ íƒí•˜ê¸°\n",
        "\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ êµ¬ê¸€ ì½”ë©ì—ì„œ ì‹¤í–‰í•œë‹¤ë©´ ë‹¤ìŒ ì½”ë“œ ì…€ì„ ì‹¤í–‰í•˜ì—¬ ì´ ë…¸íŠ¸ë¶ì—ì„œ í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼  ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "\n",
        "---\n",
        "\n",
        "ğŸ’¡ **NOTE**: ì´ ë…¸íŠ¸ë¶ì˜ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë ¤ë©´ GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. êµ¬ê¸€ ì½”ë©ì—ì„œëŠ” **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > T4 GPU**ë¥¼ ì„ íƒí•˜ì„¸ìš”.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì‚¬ìš© íŒ¨í‚¤ì§€ ë²„ì „\n",
        "\n",
        "* transformers 4.57.3\n",
        "* llama-cpp 0.3.16\n",
        "* langchain 1.2.0\n",
        "* langchain-community 0.4.1\n",
        "* langchain-openai 1.1.6\n",
        "* ddgs 9.10.0"
      ],
      "metadata": {
        "id": "YyEFN2g1FqLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹ƒí—ˆë¸Œì—ì„œ ìœ„ì ¯ ìƒíƒœ ì˜¤ë¥˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§„í–‰ í‘œì‹œì¤„ì„ ë‚˜íƒ€ë‚´ì§€ ì•Šë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.\n",
        "from transformers.utils import logging\n",
        "\n",
        "logging.disable_progress_bar()"
      ],
      "metadata": {
        "id": "ILBN7AMQ0SfM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Txh47zAxCAYd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain_community langchain_openai ddgs\n",
        "\n",
        "# ì‚¬ìš©í•˜ëŠ” íŒŒì´ì¬ê³¼ CUDA ë²„ì „ì— ë§ëŠ” llama-cpp-python íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.\n",
        "# í˜„ì¬ ì½”ë©ì˜ íŒŒì´ì¬ ë²„ì „ì€ 3.12ì´ë©° CUDA ë²„ì „ì€ 12.4(12.5)ì…ë‹ˆë‹¤.\n",
        "!pip install https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "# LLM ë¡œë“œí•˜ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4EYKJi4bCAYf",
        "outputId": "920f732f-6eb4-40a9-c50b-4de40c95c493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-02 09:48:14--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.170.185.33, 3.170.185.35, 3.170.185.14, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.170.185.33|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260102T094815Z&X-Amz-Expires=3600&X-Amz-Signature=d76a63f8f26b3e2b3279e0207b760974b1c486789f5733e3ff82ded676295563&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1767350895&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzM1MDg5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=hJWJFwTdOpBPWoHvmsjH84m4hAVSCfiCNlvBwdG6kC5zDijSSWw8Ltvjtg%7E-sn2lxYC3rorEwDaPvHaLEf6hXYDPaAa6Zzg48QJzbd3lALF4Kk4Z%7EVb0y%7ErW7IjsP7AvCG2OzJb12nkI2LVQ5xyoukhVAJQVC9ymTr4UCuD7Vuqm2J7N5qktUHTiCs3FHBz3e-keAeR1vuvjZ4%7EmZ42PzbXtA3u%7Ei4rSL12ZhnPmtVFwUWC5fKXnastH8EOClPtlAQCrqnN9NXFDoyNIlreIYuxKtE81GXfS7KKx2D-K47R%7EwShnwpDTgkScfpJZIb0FZM0eH86I27z0DU2lAPUuPg__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2026-01-02 09:48:15--  https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260102%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260102T094815Z&X-Amz-Expires=3600&X-Amz-Signature=d76a63f8f26b3e2b3279e0207b760974b1c486789f5733e3ff82ded676295563&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1767350895&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2NzM1MDg5NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=hJWJFwTdOpBPWoHvmsjH84m4hAVSCfiCNlvBwdG6kC5zDijSSWw8Ltvjtg%7E-sn2lxYC3rorEwDaPvHaLEf6hXYDPaAa6Zzg48QJzbd3lALF4Kk4Z%7EVb0y%7ErW7IjsP7AvCG2OzJb12nkI2LVQ5xyoukhVAJQVC9ymTr4UCuD7Vuqm2J7N5qktUHTiCs3FHBz3e-keAeR1vuvjZ4%7EmZ42PzbXtA3u%7Ei4rSL12ZhnPmtVFwUWC5fKXnastH8EOClPtlAQCrqnN9NXFDoyNIlreIYuxKtE81GXfS7KKx2D-K47R%7EwShnwpDTgkScfpJZIb0FZM0eH86I27z0DU2lAPUuPg__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.160.213.37, 18.160.213.16, 18.160.213.34, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.160.213.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G)\n",
            "Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G   138MB/s    in 28s     \n",
            "\n",
            "2026-01-02 09:48:43 (260 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQcht_ZFijW7",
        "outputId": "cce09dac-aa0b-48f4-b1ef-8be327bce7fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# ì—¬ëŸ¬ë¶„ì˜ ì»´í“¨í„°ì— ë‹¤ìš´ë¡œë“œí•œ ëª¨ë¸ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=4096,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3SNhQF9WthzV",
        "outputId": "1c872187-a1ba-47c0-cb8c-e91ff8779f1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "## ì²´ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# \"input_prompt\" ë³€ìˆ˜ë¥¼ ê°€ì§„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ogWsGeg6hElt"
      },
      "outputs": [],
      "source": [
        "basic_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KINQxKAINXgG",
        "outputId": "e88bee5c-e1c4-4120-9bf4-146d470dfbd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# ì²´ì¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "### ì—¬ëŸ¬ í…œí”Œë¦¿ì„ ê°€ì§„ ì²´ì¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrUKuHt_OLpe",
        "outputId": "4eb3e145-ad5e-4989-84a2-1355ea304b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-428234222.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use `RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.chains import LLMChain\n",
        "\n",
        "# ì´ì•¼ê¸° ì œëª©ì„ ìœ„í•œ ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFIyg73OtaL",
        "outputId": "df5dae82-3e34-454e-8c3f-0ab9635cf57b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of the Forgotten: A Girl\\'s Journey Through Grief\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "title.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# ìš”ì•½ê³¼ ì œëª©ì„ ì‚¬ìš©í•˜ì—¬ ìºë¦­í„° ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}.\n",
        "Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# ìš”ì•½, ì œëª©, ìºë¦­í„° ì„¤ëª…ì„ ì‚¬ìš©í•´ ì´ì•¼ê¸°ë¥¼ ìƒì„±í•˜ëŠ” ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>\n",
        "Create a story about {summary} with the title {title}.\n",
        "The main charachter is: {character}.\n",
        "Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "epNudKyyPClO"
      },
      "outputs": [],
      "source": [
        "# ì„¸ ê°œì˜ ìš”ì†Œë¥¼ ì—°ê²°í•˜ì—¬ ìµœì¢… ì²´ì¸ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "llm_chain = title | character | story"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.invoke(\"a girl that lost her mother\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJX-D-UI_1rW",
        "outputId": "fefda083-b6aa-404c-f49c-51feab0d5ce8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Finding Light in the Shadow: A Motherless Journey\"',\n",
              " 'character': ' The main character, Emily, is a resilient and compassionate young girl who embarks on an emotional journey of self-discovery after losing her mother. She navigates the complexities of grief while seeking solace in the love and support from those around her, ultimately finding strength and purpose within herself and the world she inhabits.',\n",
              " 'story': ' \"Finding Light in the Shadow: A Motherless Journey\" tells the poignant tale of Emily, a resilient and compassionate young girl who faced an unthinkable loss when her mother passed away unexpectedly. As she navigated the labyrinthine corridors of grief, Emily\\'s journey transcended mere sorrow; it became one of profound self-discovery and emotional growth. Each day presented a new challengeâ€”the lingering scent on her bedroom pillows, the empty chair at dinner, or the silence that replaced laughter in their home. Yet, through this darkness, Emily found unexpected sources of light: an empathetic teacher who saw potential beyond textbooks; friends whose support was unwavering and true; a grandmother\\'s gentle wisdom shining brightly amidst her own painâ€”all beacons guiding her towards self-compassion. Embracing this tapestry of love, Emily gradually learned to carry the legacy of her mother within herself, transforming loss into resilience and emptiness into a boundless capacity for connection. With each step forward, she unearthed strength and purpose not only in preserving her mother\\'s memory but also in shaping her own pathâ€”a testament to finding light amidst the deepest of shadows.'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# ë©”ëª¨ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-15Eoey5EJUO",
        "outputId": "4c319ba8-73af-4ded-ada3-c2cfe86c308d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten! The answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# LLMì—ê²Œ ì´ë¦„ì„ ì•Œë ¤ ì¤ë‹ˆë‹¤.\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "N42wQRl-Lykt",
        "outputId": "7a25c61e-725d-4a11-979a-2029de2035ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm unable to determine your name as I don't have the ability to access personal data about individuals.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# LLMì—ê²Œ ì´ë¦„ì„ ë¬»ìŠµë‹ˆë‹¤.\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "### ëŒ€í™” ë²„í¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# ëŒ€í™” ê¸°ë¡ì„ ë‹´ì„ ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
        "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgGMS1S9saLi",
        "outputId": "8ff53038-5685-4333-9375-40f0d7425c15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1877295073.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationBufferMemory\n",
        "\n",
        "# ì‚¬ìš©í•  ë©”ëª¨ë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mltR_GtkiqDZ",
        "outputId": "e169c379-2cec-433d-a634-aacc4448a050"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Regarding your question, the answer to 1 + 1 is indeed 2. It's always good to revisit fundamental math concepts like theseâ€”simple addition forms the foundation for more complex calculations later on. Anything else you'd like to know or discuss today?\\n\\nIf this were a standalone message:\\n\\nHello Maarten, I'm [Assistant]. The sum of 1 + 1 equals 2. If there are any other questions or topics you want to explore, feel free to ask!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# ê°„ë‹¨í•œ ì§ˆë¬¸ì„ í•˜ì—¬ ëŒ€í™” ê¸°ë¡ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-je1rmy3dx4",
        "outputId": "2c22ee46-9901-4425-c043-baf5c996c81b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units total.\\n\\n---\\n\\nIf this were part of an ongoing conversation:\\n\\nHi Maarten! My name is [Assistant]. Regarding your question, the answer to 1 + 1 is indeed 2. It's always good to revisit fundamental math concepts like theseâ€”simple addition forms the foundation for more complex calculations later on. Anything else you'd like to know or discuss today?\\n\\nIf this were a standalone message:\\n\\nHello Maarten, I'm [Assistant]. The sum of 1 + 1 equals 2. If there are any other questions or topics you want to explore, feel free to ask!\",\n",
              " 'text': \" Hello Maarten, I'm [Assistant]. The sum of 1 + 1 equals 2. If there are any other questions or topics you want to explore, feel free to ask!\\n\\nYour name is Maarten as mentioned at the beginning of this conversation.\\n\\n-----\"}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# LLMì´ ì´ë¦„ì„ ê¸°ì–µí• ê¹Œìš”?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "### ìœˆë„ ëŒ€í™” ë²„í¼"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0DRT7kjRtiC",
        "outputId": "ab261d98-0182-4e1c-fe32-e4a9b73c5482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40775140.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# ë©”ëª¨ë¦¬ì— ë§ˆì§€ë§‰ ë‘ ê°œì˜ ëŒ€í™”ë§Œ ìœ ì§€í•©ë‹ˆë‹¤.\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY69vvcR1Qq",
        "outputId": "69deb11b-4c3d-4579-81d8-b30e224a5322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\",\n",
              " 'text': ' The answer to 3 + 3 is 6. If you have any other math-related questions or need assistance, feel free to ask!'}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# ë‘ ê°œì˜ ì§ˆë¬¸ì„ ë˜ì ¸ ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "3fa66791-45d7-405b-f746-1992ac2abae5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. While there's no need for extensive personal details in this context, I'm here to help with any questions you might have!\\nHuman: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. If you have any other math-related questions or need assistance, feel free to ask!\",\n",
              " 'text': \" Your name is Maarten.\\n\\nHowever, the information you provided about your name was in the initial conversation and not directly repeated here. If you're asking from this part of our current interaction, then I don't have that detail yet as it wasn't previously mentioned after my responses.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# ì´ë¦„ì„ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "461f0229-618c-49d2-f374-e7d37851de43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. If you have any other math-related questions or need assistance, feel free to ask!\\nHuman: What is my name?\\nAI:  Your name is Maarten.\\n\\nHowever, the information you provided about your name was in the initial conversation and not directly repeated here. If you're asking from this part of our current interaction, then I don't have that detail yet as it wasn't previously mentioned after my responses.\",\n",
              " 'text': \" I'm an AI and do not have the ability to know personal information about individuals unless it has been shared with me in the course of our conversation. Therefore, I cannot determine your age without that information being provided during our interaction.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# ë‚˜ì´ë¥¼ ê¸°ì–µí•˜ëŠ”ê³  ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "### ëŒ€í™” ìš”ì•½"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lWHZlJUbwpqE"
      },
      "outputs": [],
      "source": [
        "# ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg1HAgxZMkbO",
        "outputId": "aa6803d0-d29a-4fdf-9260-5302d920d260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-385226916.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain_classic.memory import ConversationSummaryMemory\n",
        "\n",
        "# ì‚¬ìš©í•  ë©”ëª¨ë¦¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# LLM, í”„ë¡¬í”„íŠ¸, ë©”ëª¨ë¦¬ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klIk9CpVSH0",
        "outputId": "fcc3c3dd-53f4-4cf2-aafd-26194d7ff331"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': ' Greetings, Maarten! We started with a simple math problem where you asked for the sum of 1 + 1. The answer to this basic arithmetic question is 2, which stands alone and doesn\\'t require context from our conversation. As an explanation: \"Hello Maarten! It looks like we\\'re starting off with some easy-peasy addition today. When you add one (1) to another one (1), you get two (2). So, 1 + 1 = 2.\"',\n",
              " 'text': ' Your name is not explicitly mentioned in the current conversation. However, you referred to yourself as \"Maarten\" based on my response about starting with a simple math problem and then acknowledging your participation. If we\\'re considering only the information provided within this exchange, I would say that your name appears to be Maarten based on our dialogue.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# ì´ë¦„ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ” ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdOH_I-V-Fy",
        "outputId": "31c9f6b9-98b6-4ac7-c8d5-145a68c32c96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': ' The conversation began with a simple math problem where the user, referred to as \"Maarten\", asked for the sum of 1 + 1. The answer provided was 2, and an explanation was given that when you add one (1) to another one (1), it results in two (2). Later, upon inquiring about their name within the conversation context, the AI deduced that \"Maarten\" is likely the user\\'s name based on their previous interaction.\\n\\nNew summary: \\nGreetings! In our initial math problem discussion, you asked for the sum of 1 + 1, to which we answered 2. Reflecting upon your conversation with us, it appears that \"Maarten\" is probably your name as per the context provided within this particular exchange.',\n",
              " 'text': ' The first question you asked was: \"What is 1 + 1?\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# ì§€ê¸ˆê¹Œì§€ ë‚´ìš©ì´ ìš”ì•½ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_LlvrVX9HL",
        "outputId": "59dd4cb7-69fd-4d47-888c-cbadf5474143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': ' The conversation started with the user, \"Maarten\", inquiring about the sum of 1 + 1. You were correctly informed that this results in 2. Additionally, based on your interaction and context provided during the conversation, it is inferred that \"Maarten\" might be your name. Further, you asked about the first question you posed to us, which was indeed asking for the sum of 1 + 1.'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# ì§€ê¸ˆê¹Œì§€ ìš”ì•½ ë‚´ìš©ì„ í™•ì¸í•©ë‹ˆë‹¤.\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# ì—ì´ì „íŠ¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "rcBt8bZM56dM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# ë­ì²´ì¸ìœ¼ë¡œ ì˜¤í”ˆAIì˜ LLMì„ ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"MY_KEY\"\n",
        "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lmRZu8DO2p6k"
      },
      "outputs": [],
      "source": [
        "# ReAct í…œí”Œë¦¿ì„ ë§Œë“­ë‹ˆë‹¤.\n",
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin!\n",
        "\n",
        "Question: {input}\n",
        "Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=react_template,\n",
        "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NV-ssNa-4zOK"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import Tool\n",
        "from langchain_community.agent_toolkits.load_tools import load_tools\n",
        "from langchain_classic.tools import DuckDuckGoSearchResults\n",
        "\n",
        "# ì—ì´ì „íŠ¸ì— ì „ë‹¬í•  ë„êµ¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "search = DuckDuckGoSearchResults()\n",
        "search_tool = Tool(\n",
        "    name=\"duckduck\",\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run,\n",
        ")\n",
        "\n",
        "# ë„êµ¬ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤.\n",
        "tools = load_tools([\"llm-math\"], llm=openai_llm)\n",
        "\n",
        "# ê¸°ì¡´ 'Calculator' ë„êµ¬ì˜ ì„¤ëª…ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë®ì–´ì”ë‹ˆë‹¤.\n",
        "for tool in tools:\n",
        "    if tool.name == \"Calculator\":\n",
        "        tool.description = (\n",
        "            \"Useful for when you need to answer questions about math. \"\n",
        "            \"Input should be a strictly numerical mathematical expression. \"\n",
        "            \"Do NOT include any words, variable names, or currency symbols like 'price' or '$'.\"\n",
        "        )\n",
        "\n",
        "tools.append(search_tool)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "6tAr1962vS4T"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "# ReAct ì—ì´ì „íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
        "agent = create_react_agent(openai_llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSU6ECdYBOOm",
        "outputId": "1dd0eefb-d39e-4bfc-e62f-429a085371b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to find the current price of a MacBook Pro in USD and then convert it to EUR using the given exchange rate.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: 6 days ago - If a big screen is your top priority, you can score a $309 discount on the 16-inch model of the latest MacBook Pro, dropping the price to $2,190 on Amazon . It features the M4 Pro processor and 24GB of RAM for blazing-fast performance, but not ..., title: Start 2026 With a New MacBook and Save Up to $800 - CNET, link: https://www.cnet.com/deals/best-macbook-deals/, snippet: October 15, 2025 - We list their lowest price of the day All resellers: Free shipping, sales tax not included Select price for product details or to purchase Â· 14-inch M5 MacBook Pros on Holiday sale at B&H starting at $1449 , up to $150 off Appleâ€™s MSRP 12/29/2025, title: Apple 14â€³ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/14-macbook-pro/, snippet: 3 weeks ago - If you buy a product featured here, ... TL;DR: Get one of Appleâ€™s most powerful laptops, the Â· MacBook Pro, for only $399.97 (reg ...., title: You can get a MacBook Pro for $400 if you act fast | Mashable, link: https://mashable.com/article/dec-14-apple-macbook-pro-i5-2ghz-2020, snippet: October 2, 2025 - The latest 2024 16â€³ M4 Pro and M4 Max MacBook Pro prices & sales from Apple retailers, plus all the information you need to make an informed purchase Â· QuickLinks: Sales | Price Trackers | Newsletter | AppleSurfer | ğŸ” ... Donâ€™t pay full price! Here are the best sale prices as of Links below take you to retailerâ€™s product page., title: Apple 16â€³ MacBook Pro Prices at MacPrices.net, link: https://www.macprices.net/16-macbook-pro/\u001b[0m\u001b[32;1m\u001b[1;3mI found the current price of a MacBook Pro in USD, now I need to calculate the cost in EUR using the exchange rate.\n",
            "Action: Calculator\n",
            "Action Input: 2190 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1861.5\u001b[0m\u001b[32;1m\u001b[1;3mI now know the final answer\n",
            "Final Answer: The cost of a MacBook Pro in EUR would be 1861.5 EUR.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': 'The cost of a MacBook Pro in EUR would be 1861.5 EUR.'}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# ë§¥ë¶ í”„ë¡œì˜ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?\n",
        "agent_executor.invoke(\n",
        "    {\n",
        "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
        "    }\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}